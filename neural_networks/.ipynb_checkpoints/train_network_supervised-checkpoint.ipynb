{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.15\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv1D, Flatten, Dense, Activation, MaxPooling1D, Input, Dense, multiply, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import classification_report\n",
    "from random import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACNN_model():\n",
    "    inputs = Input(shape=(251,2))\n",
    "\n",
    "    conv1 = Conv1D(filters=16, kernel_size=(15), padding='same')(inputs)\n",
    "    #conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = MaxPooling1D(strides=2, pool_size=2, padding='same')(conv1)\n",
    "    \n",
    "    attention = Conv1D(filters=16, kernel_size=(15), padding='same', activation='softmax')(conv1)\n",
    "    #attention = BatchNormalization()(attention)\n",
    "    attention = Activation('relu')(attention)\n",
    "    attention = multiply([conv1, attention])\n",
    "    \n",
    "    flat = Flatten()(attention)\n",
    "    \n",
    "    dropout1 = Dropout(0.2)(flat)\n",
    "    \n",
    "    dense = Dense(units=32)(dropout1)\n",
    "    #dense = BatchNormalization()(dense)\n",
    "    dense = Activation('relu')(dense)\n",
    "    \n",
    "    dropout2 = Dropout(0.5)(dense)\n",
    "    \n",
    "    output = Dense(4, activation='softmax')(dropout2)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACNN_model()\n",
    "model.summary()\n",
    "\n",
    "#optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all data folders which allows us to programm an automated crossvalidation routine and also search for all the data folders so that they can be loaded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['beach', 'breite_strasse', 'bridge_1', 'bridge_2', 'bumblebee', 'doves', 'ducks_boat', 'ducks_children', 'golf', 'holsten_gate', 'koenigstrasse', 'puppies', 'roundabout', 'sea', 'st_petri_gate', 'st_petri_market', 'st_petri_mcdonalds', 'street']\n",
    "\n",
    "data_path = '/bigpool/strohmfn/gazecom_processed/train'\n",
    "subfolders = [f.name for f in os.scandir(data_path) if f.is_dir() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now performing 18-fold crossvalidation and therefore use the data of all but one stimuli for training and the remaining one for testing. As the data is too large for most systems we are loading ten files at a time, train on them, load the next ten and so on. The order in which the files are loaded and the data inside the files are shuffeld each epoch. After each training phase (10 files) we evaluate the performance of the network on the remaining validation set and always save the model with the lowest validation loss. After n epochs of training we evaluate the performance of the latest and the best performing model and save a classificationm report to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "export_name = 'ACNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_set in data_names:\n",
    "    best_val_los = 1000.0\n",
    "    \n",
    "    x_validate = np.load(data_path + \"/\" + val_set + \"/\" + val_set + \".npy\")\n",
    "    y_validate = np.load(data_path + \"/\" + val_set + \"/\" + val_set + \"_labels.npy\")\n",
    "    \n",
    "    print(\"Evaluation Set: \" + val_set)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(\"EPOCH: \" + str(e+1))\n",
    "\n",
    "        shuffle(subfolders)\n",
    "        \n",
    "        pbar = tqdm(total=len(subfolders))\n",
    "        \n",
    "        for i in range(0, len(subfolders), 10):\n",
    "            pbar.update(1)\n",
    "            x_train = None\n",
    "            y_train = None\n",
    "            counter = i\n",
    "            while x_train is None:\n",
    "                if val_set not in subfolders[counter]:\n",
    "                    x_train = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \".npy\")\n",
    "                    y_train = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \"_labels.npy\")\n",
    "                counter += 1\n",
    "            while counter < len(subfolders) and counter-i < 10:\n",
    "                pbar.update(1)\n",
    "                if val_set not in subfolders[counter]:\n",
    "                    x = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \".npy\")\n",
    "                    y = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \"_labels.npy\")\n",
    "                    x_train = np.concatenate((x_train,x))\n",
    "                    y_train = np.concatenate((y_train,y))\n",
    "                counter += 1\n",
    "\n",
    "            model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size, \n",
    "                            verbose=1, \n",
    "                            shuffle=True)\n",
    "\n",
    "            score = model.evaluate(x_validate, y_validate, verbose=0)\n",
    "            print(score[0])\n",
    "            print(score[1])\n",
    "            if score[0] < best_val_los:\n",
    "                best_val_los = score[0]\n",
    "                model.save_weights('best_model_' + export_name + '_' + val_set + '.h5')\n",
    "        pbar.close()   \n",
    "    model.save_weights('latest_model_' + export_name + '_' + val_set + '.h5')\n",
    "    \n",
    "    # evaluate performance of models\n",
    "    Y_validate = np.argmax(y_validate, axis=1) # Convert one-hot to index\n",
    "    target_names = ['Fixation', 'Saccade', 'Smooth Pursuit', 'Noise']\n",
    "\n",
    "    model.load_weights('latest_model_' + export_name + '_' + val_set + '.h5')\n",
    "    Y_pred = model.predict(x_validate)\n",
    "    Y_pred = np.argmax(Y_pred,axis=1)\n",
    "\n",
    "    report = classification_report(Y_validate, Y_pred, digits=5, target_names=target_names)\n",
    "    \n",
    "    text_file = open('latest_model_' + export_name + '_' + val_set + '.txt', \"w\")\n",
    "    text_file.write(report)\n",
    "    text_file.close()\n",
    "    \n",
    "    model.load_weights('best_model_' + export_name + '_' + val_set + '.h5')\n",
    "    Y_pred = model.predict(x_validate)\n",
    "    Y_pred = np.argmax(Y_pred,axis=1)\n",
    "\n",
    "    report = classification_report(Y_validate, Y_pred, digits=5, target_names=target_names)\n",
    "    \n",
    "    text_file = open('best_model_' + export_name + '_' + val_set + '.txt', \"w\")\n",
    "    text_file.write(report)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
