{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.15\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, Flatten, Dense, Activation, MaxPooling1D, Cropping1D, ZeroPadding1D, Input, multiply, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import classification_report\n",
    "from random import shuffle\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_model():\n",
    "  \n",
    "    inputs = Input(shape=(251,2))\n",
    "    \n",
    "    padding = ZeroPadding1D((3,2), input_shape = (251, 2), name='Padding')(inputs)\n",
    "\n",
    "    conv1 = Conv1D(filters=16, kernel_size=(15), activation='relu', padding='same', name='Convolution1')\n",
    "    conv1.trainable = False\n",
    "    conv1 = conv1(padding)\n",
    "    pool1 = MaxPooling1D(strides=2, pool_size=2, padding='same', name='Pooling1')(conv1)\n",
    "    \n",
    "    conv2 = Conv1D(filters=32, kernel_size=(3), activation='relu', padding='same', name='Convolution2')\n",
    "    conv2.trainable = False\n",
    "    conv2 = conv2(pool1)\n",
    "    pool2 = MaxPooling1D(strides=2, pool_size=2, padding='same', name='Pooling2')(conv2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=64, kernel_size=(3), activation='relu', padding='same', name='Convolution3')\n",
    "    conv3.trainable = False\n",
    "    conv3 = conv3(pool2)\n",
    "    pool3 = MaxPooling1D(strides=2, pool_size=2, padding='same', name='Pooling3')(conv3)\n",
    "    \n",
    "    flat = Flatten()(pool3)\n",
    "    \n",
    "    #dropout1 = Dropout(0.2)(flat)\n",
    "    \n",
    "    dense1 = Dense(units=128, name='Dense1')\n",
    "    dense1.trainable = True\n",
    "    dense1 = dense1(flat)\n",
    "    dense1 = Activation('relu')(dense1)\n",
    "    \n",
    "    #dropout2 = Dropout(0.5)(dense1)\n",
    "    \n",
    "    output = Dense(4, activation='softmax')(dense1)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, path_to_weights, path_to_architecture):\n",
    "\n",
    "    # Model reconstruction from JSON file\n",
    "    with open(path_to_architecture, 'r') as f:\n",
    "        autoencoder = model_from_json(f.read())\n",
    "    \n",
    "    # Load weights into the model\n",
    "    autoencoder.load_weights(path_to_weights)\n",
    "    \n",
    "    # extract relevant pretrained weights\n",
    "    conv1_weights = autoencoder.get_layer('conv1d_1').get_weights()\n",
    "    conv2_weights = autoencoder.get_layer('conv1d_2').get_weights()\n",
    "    conv3_weights = autoencoder.get_layer('conv1d_3').get_weights()\n",
    "    \n",
    "    # set the weights in new model\n",
    "    model.get_layer('Convolution1').set_weights(conv1_weights)\n",
    "    model.get_layer('Convolution2').set_weights(conv2_weights)\n",
    "    model.get_layer('Convolution3').set_weights(conv3_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder_model()\n",
    "model.summary()\n",
    "model = load_pretrained_weights(model, 'path_to_weights' 'path_to_architecture')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define all data folders which allows us to programm an automated crossvalidation routine and also search for all the data folders so that they can be loaded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['doves', 'ducks_boat', 'ducks_children', 'golf', 'holsten_gate', 'koenigstrasse', 'puppies', 'roundabout', 'sea', 'st_petri_gate', 'st_petri_market', 'st_petri_mcdonalds', 'street']\n",
    "\n",
    "data_path = '/bigpool/strohmfn/gazecom_processed/train'\n",
    "subfolders = [f.name for f in os.scandir(data_path) if f.is_dir() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now performing 18-fold crossvalidation and therefore use the data of all but one stimuli for training and the remaining one for testing. As the data is too large for most systems we are loading ten files at a time, train on them, load the next ten and so on. The order in which the files are loaded and the data inside the files are shuffeld each epoch. After each training phase (10 files) we evaluate the performance of the network on the remaining validation set and always save the model with the lowest validation loss. After n epochs of training we evaluate the performance of the latest and the best performing model and save a classificationm report to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "export_name = 'deep_CNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_set in data_names:\n",
    "    best_val_los = 1000.0\n",
    "    \n",
    "    x_validate = np.load(data_path + \"/\" + val_set + \"/\" + val_set + \".npy\")\n",
    "    y_validate = np.load(data_path + \"/\" + val_set + \"/\" + val_set + \"_labels.npy\")\n",
    "    \n",
    "    print(\"Evaluation Set: \" + val_set)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(\"EPOCH: \" + str(e+1))\n",
    "\n",
    "        shuffle(subfolders)\n",
    "        \n",
    "        pbar = tqdm(total=len(subfolders))\n",
    "        \n",
    "        for i in range(0, len(subfolders), 10):\n",
    "            pbar.update(1)\n",
    "            x_train = None\n",
    "            y_train = None\n",
    "            counter = i\n",
    "            while x_train is None:\n",
    "                if val_set not in subfolders[counter]:\n",
    "                    x_train = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \".npy\")\n",
    "                    y_train = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \"_labels.npy\")\n",
    "                counter += 1\n",
    "            while counter < len(subfolders) and counter-i < 10:\n",
    "                pbar.update(1)\n",
    "                if val_set not in subfolders[counter]:\n",
    "                    x = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \".npy\")\n",
    "                    y = np.load(data_path + \"/\" + subfolders[counter] + \"/\" + subfolders[counter] + \"_labels.npy\")\n",
    "                    x_train = np.concatenate((x_train,x))\n",
    "                    y_train = np.concatenate((y_train,y))\n",
    "                counter += 1\n",
    "\n",
    "            model.fit(x_train, y_train,\n",
    "                            batch_size=batch_size, \n",
    "                            verbose=1, \n",
    "                            shuffle=True)\n",
    "\n",
    "            score = model.evaluate(x_validate, y_validate, verbose=0)\n",
    "            print(score[0])\n",
    "            print(score[1])\n",
    "            if score[0] < best_val_los:\n",
    "                best_val_los = score[0]\n",
    "                model.save_weights('best_model_' + export_name + '_' + val_set + '.h5')\n",
    "        pbar.close()   \n",
    "    model.save_weights('latest_model_' + export_name + '_' + val_set + '.h5')\n",
    "\n",
    "    Y_validate = np.argmax(y_validate, axis=1) # Convert one-hot to index\n",
    "    target_names = ['Fixation', 'Saccade', 'Smooth Pursuit', 'Noise']\n",
    "\n",
    "    model.load_weights('latest_model_' + export_name + '_' + val_set + '.h5')\n",
    "    Y_pred = model.predict(x_validate)\n",
    "    Y_pred = np.argmax(Y_pred,axis=1)\n",
    "\n",
    "    report = classification_report(Y_validate, Y_pred, digits=5, target_names=target_names)\n",
    "    \n",
    "    text_file = open('latest_model_' + export_name + '_' + val_set + '.txt', \"w\")\n",
    "    text_file.write(report)\n",
    "    text_file.close()\n",
    "    \n",
    "    model.load_weights('best_model_' + export_name + '_' + val_set + '.h5')\n",
    "    Y_pred = model.predict(x_validate)\n",
    "    Y_pred = np.argmax(Y_pred,axis=1)\n",
    "\n",
    "    report = classification_report(Y_validate, Y_pred, digits=5, target_names=target_names)\n",
    "    \n",
    "    text_file = open('best_model_' + export_name + '_' + val_set + '.txt', \"w\")\n",
    "    text_file.write(report)\n",
    "    text_file.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
